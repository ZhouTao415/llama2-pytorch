# LLaMA 2 from Scratch (PyTorch) â€” RoPE Â· RMSNorm Â· MQA Â· KV Cache Â· GQA Â· SwiGLU + Inference

> Full re-implementation of core LLaMA 2 building blocks in PyTorch with clear, well-commented code and math notes. Includes multiple decoding strategies (greedy, beam search, temperature, top-k, nucleus/top-p, random sampling). PDF slides included.

[ç®€ä½“ä¸­æ–‡åœ¨ä¸‹æ–¹ â¬‡](#-ç®€ä½“ä¸­æ–‡è¯´æ˜)

![PyTorch](https://img.shields.io/badge/PyTorch-2.2+-ee4c2c)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![PRs welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)

---

## âœ¨ Features
- **Architecture**: Token/position embeddings (RoPE), **RMSNorm**, **SwiGLU** FFN  
- **Attention**: **Self-Attention with KV Cache**, **Multi-Query Attention (MQA)**, **Grouped Query Attention (GQA)**  
- **Inference**: Greedy Â· Beam Search Â· Temperature Â· Random Sampling Â· **Top-k** Â· **Top-p (Nucleus)**  
- **Math Notes**: Step-by-step derivation for **Rotary Positional Embedding (RoPE)**  
- **Utilities**: Weight loading hooks, config system, reproducible seeds, profiling helpers  
- **Slides**: PDF slides summarizing concepts and code mapping

---

## ğŸ—‚ï¸ Repository Structure

## ğŸ“ˆ Roadmap

- [ ] Multi-GPU / FSDP training script  
- [ ] FlashAttention / xFormers optional kernels  
- [ ] Quantization (int8/4) loaders  
- [ ] Export to ONNX / TensorRT  
- [ ] LoRA fine-tuning example
